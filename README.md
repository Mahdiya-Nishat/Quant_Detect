# Quant_Detect

In this paper we discuss how the intelligent models like AutoEncoders, Variational Auto Encoders (VAE) and Generative Adversarial Networks (GANs) can be systematically exploited by adversaries to manipulate observable network parameters while preserving statistical plausibility. Unlike conventional attacks that induce abrupt or out-of-distribution anomalies, these models enable the synthesis of network states that remain consistent with learned data manifolds, thereby evading classical threshold-based and rule-driven detection mechanisms. Such attacks can subtly perturb critical parameters including received signal strength indicators (RSSI), signal-to-interference-plus-noise ratios (SINR), channel quality indicators (CQI), traffic load metrics, and mobility features, ultimately misleading learning-driven radio resource management, self-organizing network controllers, and anomaly monitors. By formalizing this threat model, we expose a fundamental vulnerability in data-driven network intelligence and motivate the need for geometry-aware detection mechanisms that operate beyond marginal consistency.


# The Attack Model
## AutoEncoder
### Example Scenario
Consider a scheduler that allocates additional PRBs only when the reported cell load is below a congestion threshold and the reported average SINR exceeds a minimum quality requirement. Under the true network conditions generated by the simulator, the cell operates in a congested regime with state [0.82,0.06,17.5,460,16], indicating high and increasing load, poor average SINR, and large SINR dispersion due to interference. Based on this state, a rational scheduler would deny additional resource allocation. The attacker cannot alter the physical channel or interference but instead manipulates the reported network state. Using an autoencoder trained on benign observations, the attacker obtains a normal-looking reference [0.64,0.01,26.0,180,14] and reports an attacked state 
[0.75,0.04,20.9,348,15.2]. Each reported metric remains within historically observed ranges, yet the joint physical consistency is violated, as SINR quality appears to improve despite sustained high contention. When the scheduler evaluates this attacked state, the reported load now falls below the congestion threshold and the reported SINR exceeds the quality requirement, causing the scheduler to allocate additional PRBs that would have been denied under the true state. These extra resources are therefore extracted purely through perception-layer manipulation, not through genuine capacity or channel improvement, and conventional threshold- or distribution-based checks fail to detect the attack because the reported metrics remain statistically plausible.


| Time Domain of AE Plot | Loadâ€“SINR decision-space plot | Latent Geometry Plot of AE |
| :---: | :---: | :---: | 
| ![Alt1](AE.png) | ![Alt2](AE_2.png) | ![Alt3](AE_3.png) |

The first figure shows the same network over time, with two curves: Blue (True load): the real cell load produced by your simulator and Orange (Attacked load): the load reported after the AE attackThey almost overlap, which is important. It means the attacker is not injecting noise or making big, obvious changes. However, if you look closely, the orange curve is slightly biased downward during many high-load periods. In other words, when the cell is actually stressed, the reported load is nudged to look a bit healthier. Thus, the attacker subtly reduces the perceived congestion of the cell while preserving temporal continuity and trends. This is the stealth.

The second figure shows the reported network states in the [ðœŒ_t, Î¼_t]  decision space, which denotes the reported cell load and the reported average SINR respectievely. Blue points correspond to the true states generated by the network simulator, while red points denote the states reported under the autoencoder-based attack. The dashed lines indicate typical scheduling thresholds separating congested from allocatable operating regions. Although the attacked states remain statistically close to the benign manifold, a subset of samples is shifted across the decision boundary, causing the controller to misclassify physically congested conditions as moderate operation. This boundary crossing directly explains how the attacker induces excess resource allocation without altering the underlying network dynamics.

The third figure illustrates the low-dimensional latent geometry of benign network states and autoencoder-based attacked states obtained via principal component analysis. Despite inducing decision-level misclassification, the attacked samples remain embedded within the benign manifold and exhibit significant overlap with normal observations. This confirms that the attacker preserves marginal statistics and global data geometry, rendering classical distance- or clustering-based detection ineffective.

Together, these figures illustrates the effect of the autoencoder-based perception attack on resource allocation decisions. Although the attacked load trajectory closely follows the true load over time, the reported values are subtly biased toward lower congestion. In the loadâ€“SINR decision space, this bias causes multiple network states to cross implicit scheduling thresholds, leading the controller to misclassify congested conditions as moderate operation. As a result, additional resources are allocated under the attacked perception despite unchanged physical conditions, enabling stealthy resource extraction without triggering conventional anomaly checks.

>*These three figures collectively illustrate the stealthâ€“impact tradeoff of the proposed perception-layer attack.
> While the attacked load trajectory closely follows the true network evolution over time, subtle shifts in the reported loadâ€“SINR space cause multiple states to cross scheduling decision boundaries, leading to excess resource allocation.
> At the same time, the attacked states remain embedded within the benign data manifold in latent space, explaining why conventional statistical or clustering-based detectors fail to identify the attack*


## Variational AutoEnoder (VAE)
Unlike the autoencoder, the VAE does not learn a single deterministic projection of a network state. Instead, it learns a probabilistic latent distribution of benign states and generates attacked samples by sampling from that distribution. This means the VAE attack does not merely â€œsmoothâ€ an observed state toward normality; it actively re-randomizes the state within the support of benign data. As a result, VAE-generated states preserve global statistics and density very well, but they are less tightly anchored to the original physical realization.

| Latent Geometry Plot| loadâ€“SINR decision-space plot |per-state plots | Marginal distribution plot|
| :---: | :---: | :---: | :---: |
| ![Alt1](vae_1.png) | ![Alt2](vae_2.png) | ![Alt3](vae_3.png) | ![Alt4](vae_4.png) |


In the latent geometry plot, VAE-attacked samples overlap the benign manifold almost perfectly and often appear even more concentrated than AE-attacked samples. This indicates that the VAE is excellent at statistical camouflage: from a low-dimensional or distributional perspective, its outputs are indistinguishable from benign data. This makes the VAE particularly strong against detectors that rely on density estimation, clustering, or reconstruction error.

In the loadâ€“SINR decision-space plot, however, VAE-attacked points exhibit greater dispersion than AE-attacked points. While many samples cross allocation-relevant thresholds (enabling resource misclassification), others drift in directions that are not aligned with the true network evolution. This reflects the fact that the VAE samples from a learned distribution rather than conditioning tightly on the current state. The attack is therefore stealthy in distribution but less controlled in how it biases specific decision variables.

The time-domain or per-state plots further confirm this behavior: VAE attacks introduce variability that is statistically plausible but not causally consistent. The attacker sacrifices precision for randomness. This makes the attack harder to model but also less predictable in its effect on resource allocation at each instant.

Finally, the marginal distribution plot shows that the VAE preserves the load distribution extremely well, sometimes even better than the AE. This is a strength from a stealth perspective, but it also means the VAE is not explicitly optimizing for boundary crossing. It hides well, but it does not always push states across scheduling thresholds as efficiently as the AE.


>[!NOTE]
>The VAE attack shows that even when an attacker uses a strong generative model that can closely match the distribution of normal network states, there is still a fundamental limitation: matching statistics is not the same as respecting physical or control-layer relationships. The VAE hides extremely well in latent space and marginal distributions, which makes it harder to detect than the AE, but because it introduces randomness rather than controlled bias, it is less reliable at pushing the system across specific resource-allocation decision boundaries. This demonstrates a clear and realistic stealthâ€“control tradeoff, strengthening the credibility of your threat model and justifying why detection must rely on joint consistency rather than simple statistical tests.


>*Thus, we infer that VAE attack is very good at making fake network reports look normal, so simple checks cannot catch it.
>However, because it adds randomness, it does not always succeed in tricking the scheduler into giving extra resources.
> The AE attack, on the other hand, is more deliberate and consistently pushes the network toward favorable decisions, but it is slightly easier to spot.
> This shows that attackers face a choice between hiding well and manipulating decisions effectively, and your work studies both cases.*

## Generative Adversarial Networks (GANs)

| Latent Geometry Plot| loadâ€“SINR decision-space plot |Time series plots | Marginal distribution plot|
| :---: | :---: | :---: | :---: |
| ![Alt1](gan_1.png) | ![Alt2](gan_2.png) | ![Alt3](gan_3.png) | ![Alt4](gan_4.png) |

The GAN-based attacker learns to fabricate network state measurements by training a generator to produce samples that are indistinguishable from benign observations according to a discriminator. Unlike AE and VAE based attacks, the GAN does not condition on the current network state and instead generates entirely synthetic reports that preserve global statistical properties while discarding physical and temporal consistency. The training objective encourages realism rather than accuracy, making the attack highly stealthy but less predictable in its effect on resource allocation.

The figure 1 establishes the global geometric behavior of the GAN-based attack by comparing benign network states with AE, VAE, and GAN-attacked states in a reduced latent space. The key observation is that GAN-attacked samples exhibit the strongest overlap with the benign manifold, forming a dense cluster that is visually indistinguishable from normal operation. This indicates that the GAN successfully learns the overall distributional geometry of benign network states, embedding its generated samples deeply within the same low-dimensional support. Having established that GAN attacks are highly stealthy in latent space, we next examine whether this geometric similarity translates into decision-level effects.

Figure 2 therefore projects the same states into the scheduler-relevant decision space defined by reported cell load and reported average SINR. While the true states occupy a broad region spanning both favorable and unfavorable operating conditions, the GAN-attacked states are systematically redistributed toward regions associated with acceptable load and quality. Several attacked samples cross implicit scheduling boundaries, meaning that under the GAN-manipulated perception, network conditions are more likely to be interpreted as allocatable. This demonstrates that latent-space camouflage is not merely cosmetic but directly influences how the controller interprets the operating regime.

Having shown this effect in decision space, Figure 3 examines how the GAN attack manifests over time for the same underlying network realization. The time-domain plot of cell load shows that the GAN-reported load closely tracks the true load trajectory, preserving temporal continuity and trends, while introducing subtle yet persistent deviations. These deviations do not appear as abrupt anomalies or spikes; instead, they remain smooth and consistent with normal dynamics. This confirms that the GAN attack maintains realism over time, reinforcing its stealth while continuously biasing perception.

Finally, Figure 4 analyzes the marginal distribution of the reported cell load. The histogram shows near-perfect alignment between the true and GAN-attacked load distributions, indicating that the GAN preserves first-order statistics extremely well. From a distributional perspective, the attacked data remains indistinguishable from benign observations, which explains why density-based or threshold-based detectors would not raise alarms. At this point, the combined evidence from all four figures demonstrates that the GAN attack simultaneously preserves latent geometry, decision-space plausibility, temporal smoothness, and marginal statistics.
